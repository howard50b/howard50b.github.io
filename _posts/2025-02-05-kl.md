---
title: Common Confusions about RL for Language Models
subtitle: Here is some extra detail about the post.
layout: default
date: 2025-02-04
keywords: ai, continual learning
published: false
---

### KL Constrained RL

When applying reinforcement learning (RL) to train language models (LM), we often maximize the following objective $$J(\theta)$$:

$$\begin{align}
J(\theta) = \mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)}[R(x, y)] - \beta \text{KL}[\pi_\theta(\cdot \mid x) \mid\mid \pi_0(\cdot \mid x)]
\end{align}$$

Here, $$x$$ is the prompt, $$y$$ is the generated response, $$R(x, y)$$ is the reward function that assigns a scalar value to the entire response.


Our goal is to calculate the gradient of the objective:

$$\begin{align}
\nabla_\theta J(\theta) &= \mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)}[R(x, y)\nabla_\theta \log \pi_\theta(y \mid x)] - \beta \underbrace{\nabla_\theta \Big( \text{KL}[\pi_\theta(\cdot \mid x) \mid\mid \pi_0(\cdot \mid x)] \Big)}_{U} \\
\end{align}$$

We will expand $$U$$ in the following sections.


### Canonical Formulation

Expand $$U$$:

$$\begin{align}
U &= \nabla_\theta \sum_y \pi_\theta(y \mid x) \log \frac{\pi_\theta(y \mid x)}{\pi_0(y \mid x)} \\
&= \sum_y \nabla_\theta ~ \pi_\theta(y \mid x)\Big( \log \frac{\pi_\theta(y \mid x)}{\pi_0(y \mid x)} \Big) + \pi_\theta(y \mid x) \nabla_\theta \log \pi_\theta(y \mid x) \\
&= \sum_y \pi_\theta(y \mid x) \nabla_\theta \log \pi_\theta(y \mid x)\Big( \log \frac{\pi_\theta(y \mid x)}{\pi_0(y \mid x)} \Big) + \underbrace{\pi_\theta(y \mid x) \nabla_\theta \log \pi_\theta(y \mid x)}_{=0~~(\nabla \sum_y \pi(y \mid x) = 0)} \\
%&= \sum_y \pi_\theta(y \mid x) \nabla_\theta \log \pi_\theta(y \mid x)\Big( \log \frac{\pi_\theta(y \mid x)}{\pi_0(y \mid x)} + 1 \Big) \\
&= \mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)} \bigg[ \log \frac{\pi_\theta(y \mid x)}{\pi_0(y \mid x)} \nabla_\theta \log \pi_\theta(y \mid x) \bigg]
\end{align}$$

Now, $$U$$ can be intergrated into the original we can express in two ways equivalently:

$$\begin{align}
\nabla_\theta J(\theta) &= \mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)}[R(x, y) \nabla_\theta \log \pi_\theta(y \mid x) ] - \beta U\\
&= \mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)}\bigg[ \Big( R(x, y) - \beta \log \frac{\pi_\theta(y \mid x)}{\pi_0(y \mid x)} \Big) \nabla_\theta \log \pi_\theta(y \mid x) \bigg] \\
\mathcal{L}(\theta) &= - \frac{1}{N} \sum_{i=1}^N \bigg[ \Big( R(x_i, y_i) - \beta \log \frac{\pi_\theta(y_i \mid x_i)}{\pi_0(y_i \mid x_i)} \Big) \log \pi_\theta(y_i \mid x_i) \bigg]
\end{align}$$

This can be interpreted as adjusting the weighting of the log probability. Without the KL constraint, the weighting factor is simply $$R(x, y)$$. Now there is a term competing with the reward. If the policy needs to deviate from the reference policy by a lot to obtain the reward, this reward will be discounted (happens when $$\pi_0(y \mid x)$$ is small).

### Alternative Formulations


#### Exact

In practice, this might cause instability because $$\log \frac{\pi_\theta(y \mid x)}{\pi_0(y \mid x)}$$ is not bounded and can be either positive or negative. Another view of the same expression can be written as the following.

$$\begin{align}
U &= \mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)} \bigg[ \Big( \log \frac{\pi_\theta(y \mid x)}{\pi_0(y \mid x)} \Big) \nabla_\theta \log \pi_\theta(y \mid x) \bigg] \\
&= \mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)} \bigg[ \nabla_\theta ~ \frac{1}{2} \Big( \log \pi_\theta(y \mid x) - \log \pi_0(y \mid x) \Big)^2 \bigg] \\
\end{align}$$

This is easy to check: $$\frac{\partial}{\partial \theta} \frac{1}{2} (f(\theta) - c)^2 = (f(\theta) - c) \frac{\partial}{\partial \theta} f(\theta)$$, where we can swap in $$f(\theta) = \log \pi_\theta(y \mid x)$$ and $$c = \log \pi_0(y \mid x)$$. With this, the loss can be written as:

$$\begin{align}
\mathcal{L}(\theta) &= - \frac{1}{N} \sum_{i=1}^N  \bigg[ R(x_i, y_i) \log \pi_\theta(y_i \mid x_i) - \frac{\beta}{2} \Big( \log \pi_\theta(y_i \mid x_i) - \log \pi_0(y_i \mid x_i) \Big)^2 \bigg]
\end{align}$$

#### Approximation

In John Schulman's blog, he proposed an estimator that is slightly more biased but has a much lower variance.

$$\begin{align}
\nabla_\theta \text{KL}[\pi_\theta(\cdot \mid x) \mid\mid \pi_0(\cdot \mid x)] &\approx \nabla_\theta \sum_y \frac{\pi_0(y \mid x)}{\pi_\theta(y \mid x)} - \log \frac{\pi_0(y \mid x)}{\pi_\theta(y \mid x)} - 1 \\ 
&= \sum_y \Big( \pi_0(y \mid x) -  \pi_\theta(y \mid x) \big( \log \frac{\pi_0(y \mid x)}{\pi_\theta(y \mid x)} + 1 \big) \Big) \nabla_\theta \log \pi_\theta(y \mid x)
\end{align}$$


- R1 and DeepSeekMath chooses (r-1) - log r
- Kimi uses 1/2 logratio^2


### Implementations

There are a lot of different implementations of how the KL is incorporated into the loss.
1. [Approximating KL Divergence by John Schulman](http://joschu.net/blog/kl-approx.html)
2. [TRL](https://github.com/huggingface/trl/issues/2608)
3. [OpenRLHF]()
4. [OpenInstruct]()
5. verl
 - https://github.com/volcengine/verl/issues/211
 - https://github.com/volcengine/verl/blob/main/verl/trainer/ppo/ray_trainer.py#L908

> Also note that, instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss, avoiding complicating the calculation of $$A$$.
### Token Level vs Response Level KL
...

### Clipping in PPO

[Understanding why there isn't a log probability in TRPO and PPO's objective](https://costa.sh/blog-understanding-why-there-isn't-a-log-probability-in-trpo-and-ppo's-objective)

$$\begin{align}
\nabla_\theta J(\theta) &= \mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)} \Big[ A(x, y) \nabla_\theta \log \pi_\theta(y \mid x) \Big] \\
&= \mathbb{E}_{y \sim \pi'(\cdot \mid x)} \bigg[ \frac{ \pi_\theta(y \mid x) }{ \pi'(y \mid x) } A(x, y) \nabla_\theta \log \pi_\theta(y \mid x) \bigg] \\
&= \mathbb{E}_{y \sim \pi'(\cdot \mid x)} \bigg[\frac{\nabla_\theta \pi_\theta(y \mid x)}{\pi'(y \mid x)} A(x, y) \bigg] \\
\mathcal{L}(\theta) &= - \frac{1}{N} \sum_{i=1}^N \bigg[ \frac{\pi_\theta(y_i \mid x_i)}{\pi'(y_i \mid x_i)} A(x_i, y_i) \bigg]
\end{align}$$

### Conclusion
...



```latex
@article{chen2025RLforLM,
   title={Details about RL for Language Models},
   author={Howard Chen},
   year={2025}
}
```
