---
title: Home
layout: home
---

<h1 style="font-size: 26px; margin-bottom: 1.5rem; font-weight: normal;">Howard Chen</h1>

<div class="home-content" style="display: flex; gap: 2.5rem; align-items: flex-start;">
<div class="home-bio" style="flex: 1;" markdown="1">

I recently received my PhD in CS from [Princeton](https://pli.princeton.edu/), where I was co-advised by [Danqi Chen](https://www.cs.princeton.edu/~danqic/) and [Karthik Narasimhan](https://www.cs.princeton.edu/~karthikn/).

**My research focuses on building safe agents that can operate reliably on a long interactive horizon and can continually improve.**

Towards this goal, my research covers several topics:
<div class="callout" markdown="1">
- Building large-scale virtual and embodied environments to train and evaluate agents ([WebShop](https://arxiv.org/abs/2207.01206), [Touchdown](https://arxiv.org/abs/1811.12354)).
- Building long-term memory and developing algorithms for continual learning ([MemWalker](https://arxiv.org/abs/2310.05029), [Continual Memorization](https://arxiv.org/abs/2411.07175)).
- Understanding the properties of post-training algorithms ([RL Forgets Less than SFT](https://arxiv.org/abs/2510.18874)).
- Interpretability & safety ([Interpretability Improves Robustness](https://arxiv.org/abs/2204.11790), [Belief Shift under Growing Context](https://arxiv.org/abs/2511.01805)).
- AI for advancing science ([AI Reverse-Engineering Blackboxes](https://arxiv.org/abs/2505.17968), [AI Science Tutor](https://arxiv.org/abs/2402.11111)), and benchmarking multi-modal reasoning on scientific knowledge/charts ([CharXiv](https://arxiv.org/abs/2406.18521)).
</div>

<!--
**My research has been focusing on building AI systems that can handle long-horizon and continual tasks.**
More broadly, I aim to build lifelong agents that can operate reliably over long time horizons and self-improve.
I approach it through the lens of *memory* in both its in-context and parametric forms: 
designing challenging evaluation to stress test long-horizon agentic capabilities ([WebShop](https://arxiv.org/abs/2207.01206)),
developing algorithms that let models self-organize and manage its long-term memory ([MemWalker](https://arxiv.org/abs/2310.05029)),
examining reliability under context accumulation ([Tracking Belief Shift](https://arxiv.org/abs/2511.01805)),
and understading how model parameters can be updated without drastic forgetting of old knowledge/capabilities ([Continual Memorization](https://arxiv.org/abs/2411.07175), [RL Mitigates Forgetting](https://arxiv.org/abs/2510.18874)).
I also spent quite some time thinking about interpretability and safety from the memory perspective.
-->

During my PhD, I have interned at Meta (FAIR) working with [Asli Celikyilmaz](http://asli.us/) and [Jason Weston](https://www.thespermwhale.com/jaseweston/).
Prior to Princeton, I was an ML researcher at [ASAPP](https://www.asapp.com/) working with [Tao Lei](https://taolei87.github.io/). I was also a research assistant at Cornell Tech working with [Yoav Artzi](https://yoavartzi.com/).

I recieved my M.Eng. in CS from Cornell and B.S. in EE from National Taiwan University.

</div>
<div class="home-photo" style="width: 32%; flex-shrink: 0;">
<img src="/assets/howardchen.png" style="width: 100%;">
<p style="font-family: courier; font-size: 15px; margin: 0rem 0;">howardbchen@gmail.com</p>
<div style="font-size: 14px; margin-top: 0rem;">
<a href="https://scholar.google.com/citations?user=wsNa_W4AAAAJ&hl=en&authuser=3">Google Scholar</a> &nbsp;|&nbsp; 
<a href="https://github.com/howard50b">GitHub</a> &nbsp;|&nbsp;
<a href="https://x.com/__howardchen">Twitter</a>
</div>
</div>
</div>
